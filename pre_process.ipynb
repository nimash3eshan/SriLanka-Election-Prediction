{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and environment loaded.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: IMPORTS AND SETUP\n",
    "\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from deep_translator import GoogleTranslator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time # <-- IMPORTED FOR ERROR HANDLING\n",
    "import ast \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported and environment loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: CONFIGURATION AND CONSTANTS\n",
    "\n",
    "# --- API and Data Files ---\n",
    "BEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n",
    "RAW_TWEETS_CSV = 'sri_lanka_election_tweets_raw.csv'\n",
    "FINAL_PROCESSED_CSV = 'sri_lanka_election_tweets_final_processed.csv'\n",
    "\n",
    "\n",
    "# --- Search and Attribution ---\n",
    "politician_keywords = {\n",
    "    'Anura Kumara Dissanayake': ['anura kumara', 'akd', '@anuradissanayake', 'anuradissanayake'],\n",
    "    'Sajith Premadasa': ['sajith premadasa', 'sajith', '@sajithpremadasa'],\n",
    "    'Ranil Wickremesinghe': ['ranil wickremesinghe', 'ranil', '@RW_UNP'],\n",
    "    'Namal Rajapaksa': ['namal rajapaksa', 'namal', '@RajapaksaNamal']\n",
    "}\n",
    "SEARCH_QUERY = '(\"Anura Kumara\" OR AKD OR \"Sajith\" OR \"Ranil\" OR \"Namal Rajapaksa\") (lang:en OR lang:si) -is:retweet'\n",
    "TWEET_LIMIT = 2500 # Increased limit for a more robust dataset\n",
    "\n",
    "\n",
    "# --- Sentiment Dictionaries ---\n",
    "emoticons_happy = set([':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P', 'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'])\n",
    "emoticons_sad = set([':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c', ':c', ':{', '>:\\\\', ';('])\n",
    "singlish_happy = set(['hondai', 'honday', 'hondaiy', 'hondaii', 'niyamai', 'niyamay', 'supiri', 'supiriyak', 'supiriii', 'patta', 'maru', 'shok', 'shoi', 'ela', 'elakiri', 'elaa', 'jayawewa', 'jaya wewa', 'lassanai', 'lassanay', 'gammak', 'gammac', 'sira', 'siraa', 'ow', 'owu', 'ov', 'hari', 'aththa', 'aththac', 'subapathum', 'suba pathum', 'pissu kora', 'thanks', 'thankz', 'thnx', 'tnx'])\n",
    "singlish_sad = set(['narakai', 'narakay', 'boru', 'boruwak', 'boruu', 'weradi', 'waradi', 'varadi', 'weradii', 'chater', 'chaater', 'chaa', 'epaa', 'epa', 'hora', 'horu', 'horakam', 'pissu', 'pisso', 'gon', 'gonn', 'gon haraka', 'pal horu', 'kalakanni', 'pala', 'palayan', 'aiyo', 'aiyoo', 'ane', 'apoi', 'ammapa', 'na', 'naa', 'ne', 'naha', 'nathuwa', 'nathi', 'neti', 'nathe'])\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: HELPER FUNCTIONS\n",
    "\n",
    "def get_target_politicians(text, keywords_dict):\n",
    "    \"\"\"Identifies which politician(s) are mentioned in a tweet.\"\"\"\n",
    "    mentioned = []\n",
    "    text_lower = text.lower()\n",
    "    for politician, keywords in keywords_dict.items():\n",
    "        if any(keyword in text_lower for keyword in keywords):\n",
    "            mentioned.append(politician)\n",
    "    # If no specific politician is found, you might want to label it as 'General' or skip\n",
    "    return mentioned if mentioned else []\n",
    "\n",
    "def clean_text_for_blob(tweet_text):\n",
    "    \"\"\"Minimal cleaning for TextBlob.\"\"\"\n",
    "    tweet_text = re.sub(r'https?:\\/\\/\\S+', '', tweet_text)\n",
    "    tweet_text = re.sub(r'@[A-Za-z0-9_]+', '', tweet_text)\n",
    "    tweet_text = re.sub(r'#', '', tweet_text)\n",
    "    return tweet_text\n",
    "\n",
    "def calculate_custom_polarity(tweet_text):\n",
    "    \"\"\"Calculates custom polarity using Singlish words and emoticons as modifiers.\"\"\"\n",
    "    polarity = 0.0\n",
    "    text_lower = tweet_text.lower()\n",
    "    for word in text_lower.split():\n",
    "        if word in singlish_happy: polarity += 0.1\n",
    "        elif word in singlish_sad: polarity -= 0.1\n",
    "    for emoticon in emoticons_happy:\n",
    "        if emoticon in text_lower: polarity += 0.1\n",
    "    for emoticon in emoticons_sad:\n",
    "        if emoticon in text_lower: polarity -= 0.1\n",
    "    return max(min(polarity, 1.0), -1.0)\n",
    "\n",
    "def get_hybrid_sentiment(text_for_blob, custom_polarity):\n",
    "    \"\"\"Combines TextBlob and custom polarities, then clamps the result.\"\"\"\n",
    "    try:\n",
    "        analysis = TextBlob(text_for_blob)\n",
    "        textblob_polarity = analysis.sentiment.polarity\n",
    "        subjectivity = analysis.sentiment.subjectivity\n",
    "    except Exception:\n",
    "        textblob_polarity = 0.0\n",
    "        subjectivity = 0.0\n",
    "    hybrid_polarity = textblob_polarity + custom_polarity\n",
    "    hybrid_polarity = max(min(hybrid_polarity, 1.0), -1.0)\n",
    "    if hybrid_polarity > 0.05: sentiment = 'Positive'\n",
    "    elif hybrid_polarity < -0.05: sentiment = 'Negative'\n",
    "    else: sentiment = 'Neutral'\n",
    "    return sentiment, hybrid_polarity, subjectivity\n",
    "\n",
    "def translate_text(text, target_lang='en'):\n",
    "    \"\"\"Translates text, returning original on failure.\"\"\"\n",
    "    try:\n",
    "        if not text or not isinstance(text, str): return text\n",
    "        return GoogleTranslator(source='auto', target=target_lang).translate(text) or text\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data file not found. Starting fresh extraction from Twitter...\n",
      "Attempting to fetch tweets...\n",
      "--- AN ERROR OCCURRED: 401 Unauthorized\n",
      "Unauthorized ---\n",
      "Waiting for 60 seconds before retrying...\n",
      "Attempting to fetch tweets...\n",
      "--- AN ERROR OCCURRED: 401 Unauthorized\n",
      "Unauthorized ---\n",
      "Waiting for 120 seconds before retrying...\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: DATA EXTRACTION WITH ROBUST ERROR HANDLING\n",
    "\n",
    "# Check if the raw data file already exists to avoid re-running\n",
    "if os.path.exists(RAW_TWEETS_CSV):\n",
    "    print(f\"Raw data file '{RAW_TWEETS_CSV}' found. Loading from disk.\")\n",
    "    df_raw = pd.read_csv(RAW_TWEETS_CSV)\n",
    "else:\n",
    "    print(f\"Raw data file not found. Starting fresh extraction from Twitter...\")\n",
    "    if not BEARER_TOKEN:\n",
    "        raise Exception(\"BEARER_TOKEN not found in environment. Please check your .env file.\")\n",
    "\n",
    "    client = tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)\n",
    "    raw_tweet_data = []\n",
    "    \n",
    "    # --- NEW: Robust retry loop with exponential backoff ---\n",
    "    backoff_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Attempting to fetch tweets...\")\n",
    "            # Using tweepy's Paginator to handle response pages automatically\n",
    "            for i, tweet in enumerate(tweepy.Paginator(client.search_recent_tweets,\n",
    "                                                     query=SEARCH_QUERY,\n",
    "                                                     tweet_fields=[\"id\", \"text\", \"created_at\", \"source\", \"lang\", \"public_metrics\", \"possibly_sensitive\", \"author_id\"],\n",
    "                                                     expansions=[\"author_id\"],\n",
    "                                                     max_results=100).flatten(limit=TWEET_LIMIT)):\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"...{i + 1} tweets fetched\")\n",
    "\n",
    "                raw_tweet_data.append({\n",
    "                    'id': tweet.id,\n",
    "                    'created_at': tweet.created_at,\n",
    "                    'original_text': tweet.text,\n",
    "                    'lang': tweet.lang,\n",
    "                    'source': tweet.source,\n",
    "                    'favorite_count': tweet.public_metrics.get('like_count', 0),\n",
    "                    'retweet_count': tweet.public_metrics.get('retweet_count', 0)\n",
    "                })\n",
    "\n",
    "            print(\"Successfully completed tweet fetching.\")\n",
    "            break  # Exit the while loop on success\n",
    "\n",
    "        # Catching the specific Tweepy exception for network errors, rate limits, etc.\n",
    "        except tweepy.errors.TweepyException as e:\n",
    "            sleep_duration = 60 * backoff_counter\n",
    "            print(f\"--- AN ERROR OCCURRED: {e} ---\")\n",
    "            print(f\"Waiting for {sleep_duration} seconds before retrying...\")\n",
    "            time.sleep(sleep_duration)\n",
    "            backoff_counter += 1  # Increase wait time for the next potential error\n",
    "            continue # Retry the fetch\n",
    "\n",
    "    # --- Save the raw data so we don't have to do this again ---\n",
    "    df_raw = pd.DataFrame(raw_tweet_data)\n",
    "    df_raw.to_csv(RAW_TWEETS_CSV, index=False, encoding='utf-8')\n",
    "    print(f\"\\nExtraction complete. {len(df_raw)} tweets saved to '{RAW_TWEETS_CSV}'\")\n",
    "\n",
    "print(\"\\nRaw DataFrame shape:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
