{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fd16cccd-88bf-4dfb-91cb-984452ec8acc",
    "_uuid": "25df537f-aa9f-4b97-9e91-b4a1e60500bb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T14:47:56.602634Z",
     "iopub.status.busy": "2025-06-21T14:47:56.602415Z",
     "iopub.status.idle": "2025-06-21T14:50:24.229273Z",
     "shell.execute_reply": "2025-06-21T14:50:24.222265Z",
     "shell.execute_reply.started": "2025-06-21T14:47:56.602611Z"
    },
    "executionInfo": {
     "elapsed": 94554,
     "status": "ok",
     "timestamp": 1750186327968,
     "user": {
      "displayName": "Cybertron Ultra",
      "userId": "09045234525718726515"
     },
     "user_tz": -330
    },
    "id": "4qUFfRoBOSAV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "0fab23de-4194-43ee-ce4f-980c270042d4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas \\\n",
    "matplotlib \\\n",
    "seaborn \\\n",
    "textblob \\\n",
    "deep-translator \\\n",
    "ipywidgets \\\n",
    "scikit-learn \\\n",
    "xgboost \\\n",
    "tensorflow=='2.16.1' \\\n",
    "transformers \\\n",
    "notebook \\\n",
    "bertopic \\\n",
    "tf-keras \\\n",
    "polars \\\n",
    "emoji \\\n",
    "pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "448c18fc-edf4-4c25-b1d2-680e2db62dd3",
    "_uuid": "e6386906-ce76-4c39-bc09-ae51c5a91d05",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T14:50:24.231279Z",
     "iopub.status.busy": "2025-06-21T14:50:24.231026Z",
     "iopub.status.idle": "2025-06-21T14:51:27.761726Z",
     "shell.execute_reply": "2025-06-21T14:51:27.757765Z",
     "shell.execute_reply.started": "2025-06-21T14:50:24.231254Z"
    },
    "id": "tSgx--c-Oqu3",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for ML pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Models\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f5e66e6d-6268-480e-9998-ef11ce085d4d",
    "_uuid": "16bec048-9789-4a64-864e-01d273099775",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:17:35.347542Z",
     "iopub.status.busy": "2025-06-21T15:17:35.347212Z",
     "iopub.status.idle": "2025-06-21T15:17:35.380912Z",
     "shell.execute_reply": "2025-06-21T15:17:35.376443Z",
     "shell.execute_reply.started": "2025-06-21T15:17:35.347515Z"
    },
    "id": "Cv5XYngROwEx",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset using Polars\n",
    "try:\n",
    "    df = pl.read_csv('/kaggle/input/tweet-politicians/sri_lanka_election_tweets_analysis_ready.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'merged_data_2.csv' not found. Please upload your dataset.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Data head:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1627c821-bc53-4fd8-94db-e1d1d517a8a2",
    "_uuid": "1cd62113-23fb-4058-a181-775d4ef5fad1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:17:47.881763Z",
     "iopub.status.busy": "2025-06-21T15:17:47.881460Z",
     "iopub.status.idle": "2025-06-21T15:17:47.896871Z",
     "shell.execute_reply": "2025-06-21T15:17:47.891871Z",
     "shell.execute_reply.started": "2025-06-21T15:17:47.881738Z"
    },
    "id": "j9GopG0EOyD4",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Domain-Specific Lexicons (Singlish/Colloquial)\n",
    "singlish_happy = {\n",
    "    'patta': 2, 'maru': 2, 'niyamai': 2, 'ela': 2, 'supiri': 2, 'gathi': 2,\n",
    "    'hodai': 1, 'lassanai': 1, 'shok': 1, 'jayawewa': 2, 'good':1, 'love':2\n",
    "}\n",
    "singlish_sad = {\n",
    "    'apalai': -2, 'chaa': -2, 'anthimai': -2, 'weda na': -2, 'boring': -1,\n",
    "    'boru': -1, 'harak': -2, 'gon': -2, 'pissu': -2, 'sad':-1,'bad':-1\n",
    "}\n",
    "\n",
    "# 2. Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans tweet text by removing URLs, mentions, hashtags, and non-alphanumeric characters.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtag symbols but keep the text\n",
    "    text = emoji.demojize(text) # Convert emojis to text (e.g., ❤️ -> :red_heart:)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove non-alphabetic characters except spaces\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# 3. Custom Polarity Calculation Function\n",
    "def calculate_custom_polarity(text):\n",
    "    \"\"\"Calculates polarity based on the custom Singlish lexicons.\"\"\"\n",
    "    score = 0\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        score += singlish_happy.get(word, 0)\n",
    "        score += singlish_sad.get(word, 0)\n",
    "    return score\n",
    "\n",
    "# 4. Hybrid Sentiment Calculation Function\n",
    "def get_hybrid_sentiment(text, custom_score):\n",
    "    \"\"\"\n",
    "    Combines TextBlob's general sentiment with our custom score.\n",
    "    This gives more weight to our domain-specific lexicon.\n",
    "    \"\"\"\n",
    "    textblob_polarity = TextBlob(text).sentiment.polarity\n",
    "    # Simple averaging, but can be weighted if desired\n",
    "    # We will give our custom score more importance\n",
    "    hybrid_score = (textblob_polarity * 0.4) + (custom_score * 0.6)\n",
    "    return max(-1.0, min(1.0, hybrid_score)) # Clamp the score between -1 and 1\n",
    "\n",
    "print(\"✅ Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8572f4a4-952a-403a-90cb-d912cba9b7eb",
    "_uuid": "063a31f6-0414-4271-8d4c-ebaa81c0b36a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:18:20.891732Z",
     "iopub.status.busy": "2025-06-21T15:18:20.891400Z",
     "iopub.status.idle": "2025-06-21T15:18:21.506931Z",
     "shell.execute_reply": "2025-06-21T15:18:21.503412Z",
     "shell.execute_reply.started": "2025-06-21T15:18:20.891681Z"
    },
    "id": "OkRMEN6kO0WF",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This cell applies all the feature engineering steps using Polars expressions.\n",
    "# Polars' expression-based API is highly optimized and much faster than row-by-row apply in pandas.\n",
    "\n",
    "df_featured = df.with_columns([\n",
    "    # Step 1: Clean the tweet text\n",
    "    pl.col(\"original_text\").map_elements(clean_text, return_dtype=pl.String).alias(\"clean_text\"),\n",
    "\n",
    "    # Step 2: Calculate engagement score (using a log transform to handle large variations)\n",
    "    # We add 1 to avoid log(0). Retweets are often more valuable than likes.\n",
    "    (pl.col(\"favorite_count\") + (pl.col(\"retweet_count\") * 1.5) + 1).log().alias(\"engagement_score\")\n",
    "]).with_columns([\n",
    "    # Step 3: Calculate custom and hybrid polarity on the cleaned text\n",
    "    pl.col(\"clean_text\").map_elements(calculate_custom_polarity, return_dtype=pl.Float64).alias(\"custom_polarity\"),\n",
    "    pl.col(\"clean_text\").map_elements(lambda x: TextBlob(x).sentiment.polarity, return_dtype=pl.Float64).alias(\"textblob_polarity\")\n",
    "]).with_columns([\n",
    "    # Step 4: Combine into a hybrid score\n",
    "    ((pl.col(\"textblob_polarity\") * 0.4) + (pl.col(\"custom_polarity\") * 0.6)).alias(\"hybrid_polarity\")\n",
    "]).with_columns([\n",
    "    # Step 5: This is our TARGET variable! The final weighted impact score.\n",
    "    (pl.col(\"hybrid_polarity\") * pl.col(\"engagement_score\")).alias(\"weighted_polarity\")\n",
    "])\n",
    "\n",
    "print(\"Feature engineering complete. Final DataFrame schema:\")\n",
    "print(df_featured.schema)\n",
    "print(\"\\nSample of the engineered data:\")\n",
    "print(df_featured.select([\n",
    "    \"clean_text\", \"hybrid_polarity\", \"engagement_score\", \"weighted_polarity\", \"target_politicians\"\n",
    "]).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2084ba9e-6494-49e2-a438-a395f12f3645",
    "_uuid": "5adf8437-78ae-4eaa-86aa-c1ca70f6154f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:37:38.387069Z",
     "iopub.status.busy": "2025-06-21T15:37:38.386747Z",
     "iopub.status.idle": "2025-06-21T15:37:38.422116Z",
     "shell.execute_reply": "2025-06-21T15:37:38.417250Z",
     "shell.execute_reply.started": "2025-06-21T15:37:38.387041Z"
    },
    "id": "P_zk8nF4O2BJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define our features (X) and target (y)\n",
    "# Features: The cleaned text and the raw engagement numbers\n",
    "# Target: The weighted_polarity score we just created\n",
    "\n",
    "# Note: We convert to pandas here because scikit-learn's train_test_split\n",
    "# and ColumnTransformer have better integration with pandas DataFrames.\n",
    "# The heavy lifting (feature engineering) has already been done efficiently by Polars.\n",
    "pandas_df = df_featured.to_pandas()\n",
    "\n",
    "X = pandas_df[['clean_text', 'favorite_count', 'retweet_count', 'hybrid_polarity', 'target_politicians']]\n",
    "y = pandas_df['weighted_polarity']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06c57a84-1b13-4a15-a7b6-58fc6e99cf23",
    "_uuid": "3bf6f574-bc81-4e18-a6e6-fced10413100",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:57:05.705056Z",
     "iopub.status.busy": "2025-06-21T15:57:05.704745Z",
     "iopub.status.idle": "2025-06-21T15:57:06.412622Z",
     "shell.execute_reply": "2025-06-21T15:57:06.407111Z",
     "shell.execute_reply.started": "2025-06-21T15:57:05.705030Z"
    },
    "id": "ZMilb0TjO4Ab",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing steps for different column types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)), 'clean_text'),\n",
    "        ('numeric', StandardScaler(), ['favorite_count', 'retweet_count', 'hybrid_polarity'])\n",
    "    ],\n",
    "    # remainder='passthrough' # Keep other columns if any\n",
    ")\n",
    "\n",
    "# Create the full pipeline with the preprocessor and the XGBoost Regressor\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=1000,          # More trees\n",
    "        learning_rate=0.05,         # Lower learning rate\n",
    "        max_depth=5,                # Deeper trees\n",
    "        subsample=0.8,              # Use 80% of data per tree\n",
    "        colsample_bytree=0.8,       # Use 80% of features per tree\n",
    "        random_state=42,\n",
    "        n_jobs=-1,                  # Use all available CPU cores\n",
    "        early_stopping_rounds=50    # Stop training if validation score doesn't improve\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "# The 'eval_set' is used for early stopping to prevent overfitting\n",
    "xgb_pipeline.named_steps['preprocessor'].fit(X_test)\n",
    "eval_set = [(xgb_pipeline.named_steps['preprocessor'].transform(X_test), y_test)]\n",
    "xgb_pipeline.fit(X_train, y_train, regressor__eval_set=eval_set, regressor__verbose=False)\n",
    "\n",
    "import pickle\n",
    "with open('xgb_pipeline_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_pipeline, f)\n",
    "\n",
    "print(\"✅ XGBoost model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ae79675-e45e-402b-92b1-97f6122a3601",
    "_uuid": "ea64ee43-4c93-4aff-baa0-2f390afa9df1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:38:45.087594Z",
     "iopub.status.busy": "2025-06-21T15:38:45.087240Z",
     "iopub.status.idle": "2025-06-21T15:38:45.310735Z",
     "shell.execute_reply": "2025-06-21T15:38:45.305075Z",
     "shell.execute_reply.started": "2025-06-21T15:38:45.087565Z"
    },
    "id": "jdGWIiwRO6LD",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"--- XGBoost Model Evaluation ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_xgb:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_xgb:.4f}\")\n",
    "print(f\"R-squared (R²): {r2_xgb:.4f}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=y_test, y=y_pred_xgb, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)\n",
    "plt.title('XGBoost: True vs. Predicted Impact Score')\n",
    "plt.xlabel('True Weighted Polarity')\n",
    "plt.ylabel('Predicted Weighted Polarity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b307cfc-016e-4c58-82e5-b98fdd96cd3c",
    "_uuid": "c0b0537e-c203-4a60-bc3e-d6a32d785b99",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T14:54:45.253685Z",
     "iopub.status.busy": "2025-06-21T14:54:45.253311Z",
     "iopub.status.idle": "2025-06-21T14:54:54.888661Z",
     "shell.execute_reply": "2025-06-21T14:54:54.883784Z",
     "shell.execute_reply.started": "2025-06-21T14:54:45.253656Z"
    },
    "id": "tlNA2Xb8O8FV",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NN-specific data preparation\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize the text data\n",
    "X_train_tokens = tokenizer(\n",
    "    X_train['clean_text'].tolist(),\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='tf'\n",
    ")\n",
    "X_test_tokens = tokenizer(\n",
    "    X_test['clean_text'].tolist(),\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Scale numeric features for the NN\n",
    "numeric_scaler = StandardScaler()\n",
    "X_train_numeric = numeric_scaler.fit_transform(X_train[['favorite_count', 'retweet_count', 'hybrid_polarity']])\n",
    "X_test_numeric = numeric_scaler.transform(X_test[['favorite_count', 'retweet_count', 'hybrid_polarity']])\n",
    "\n",
    "# Function to create the hybrid model\n",
    "def create_hybrid_regressor():\n",
    "    # Transformer part for text\n",
    "    bert_model = TFBertModel.from_pretrained(MODEL_NAME, trainable=True) # Freeze BERT layers for faster training\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0] # Use the [CLS] token output\n",
    "    text_features = tf.keras.layers.GlobalAveragePooling1D()(bert_output)\n",
    "\n",
    "    # Numeric part\n",
    "    numeric_input = tf.keras.layers.Input(shape=(X_train_numeric.shape[1],), name='numeric_input')\n",
    "\n",
    "    # Concatenate text and numeric features\n",
    "    concatenated = tf.keras.layers.Concatenate()([text_features, numeric_input])\n",
    "\n",
    "    # Dense layers for regression\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(concatenated)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='linear', name='output')(x) # Linear activation for regression\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask, numeric_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss='mean_squared_error', # Regression loss\n",
    "                  metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "nn_model = create_hybrid_regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b2836188-6847-4f10-956b-4d39a18dcd60",
    "_uuid": "62d3875a-09d0-49a3-84da-9d384e27967d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T14:55:22.007906Z",
     "iopub.status.busy": "2025-06-21T14:55:22.007530Z",
     "iopub.status.idle": "2025-06-21T15:07:06.483721Z",
     "shell.execute_reply": "2025-06-21T15:07:06.478586Z",
     "shell.execute_reply.started": "2025-06-21T14:55:22.007876Z"
    },
    "id": "FaT_C9gEO-PX",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare the input dictionary for the model\n",
    "X_train_nn = {\n",
    "    'input_ids': X_train_tokens['input_ids'],\n",
    "    'attention_mask': X_train_tokens['attention_mask'],\n",
    "    'numeric_input': X_train_numeric\n",
    "}\n",
    "X_test_nn = {\n",
    "    'input_ids': X_test_tokens['input_ids'],\n",
    "    'attention_mask': X_test_tokens['attention_mask'],\n",
    "    'numeric_input': X_test_numeric\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(\n",
    "    X_train_nn,\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10, # Increase epochs for better performance, but 10 is good for a demo\n",
    "    batch_size=32,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Neural Network model training complete.\")\n",
    "\n",
    "# Evaluate the NN model\n",
    "y_pred_nn = nn_model.predict(X_test_nn).flatten()\n",
    "\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(\"\\n--- Neural Network Model Evaluation ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_nn:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_nn:.4f}\")\n",
    "print(f\"R-squared (R²): {r2_nn:.4f}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=y_test, y=y_pred_nn, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)\n",
    "plt.title('Neural Network: True vs. Predicted Impact Score')\n",
    "plt.xlabel('True Weighted Polarity')\n",
    "plt.ylabel('Predicted Weighted Polarity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1a6f137d-7f7e-4fea-b6f5-3bee863bd5b6",
    "_uuid": "a9ac2473-9924-4e0a-b27e-194d24010a12",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:10:45.632943Z",
     "iopub.status.busy": "2025-06-21T15:10:45.632603Z",
     "iopub.status.idle": "2025-06-21T15:10:46.067713Z",
     "shell.execute_reply": "2025-06-21T15:10:46.061935Z",
     "shell.execute_reply.started": "2025-06-21T15:10:45.632917Z"
    },
    "id": "6_-g0ACcPAbf",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract the preprocessor and regressor from the pipeline\n",
    "preprocessor = xgb_pipeline.named_steps['preprocessor']\n",
    "regressor = xgb_pipeline.named_steps['regressor']\n",
    "\n",
    "# Get feature names from the TfidfVectorizer and the numeric columns\n",
    "text_features = preprocessor.named_transformers_['text'].get_feature_names_out()\n",
    "numeric_features = preprocessor.named_transformers_['numeric'].get_feature_names_out()\n",
    "\n",
    "# Combine all feature names in the correct order\n",
    "all_features = np.concatenate([text_features, numeric_features])\n",
    "\n",
    "# Get the feature importances from the trained XGBoost model\n",
    "importances = regressor.feature_importances_\n",
    "\n",
    "# Create a Polars DataFrame for easy sorting and plotting\n",
    "importance_df = pl.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': importances\n",
    "}).sort('importance', descending=True)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Plot the top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20 = importance_df.head(20).to_pandas() # Matplotlib works best with pandas\n",
    "sns.barplot(x='importance', y='feature', data=top_20, palette='viridis')\n",
    "plt.title('Top 20 Feature Importances for XGBoost Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae67bad4-3cb3-4070-ba7f-e9f728d1b943",
    "_uuid": "514dfa37-b900-44bb-a67a-2c134a2e89ee",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "=================================== option 2 ====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1900c949-1f9e-407c-8633-a00809256fc5",
    "_uuid": "88f91695-88df-4fea-a89e-8ad90a3fa88d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:11:03.922178Z",
     "iopub.status.busy": "2025-06-21T15:11:03.921873Z",
     "iopub.status.idle": "2025-06-21T15:11:04.042681Z",
     "shell.execute_reply": "2025-06-21T15:11:04.038357Z",
     "shell.execute_reply.started": "2025-06-21T15:11:03.922154Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for ML pipeline and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Hugging Face Transformers and TensorFlow\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "178088ed-4ac7-4871-b7ef-33308e7331fd",
    "_uuid": "e039bf23-d253-4993-b164-e75d5bdb2f3b",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-06-21T14:51:35.716169Z",
     "iopub.status.idle": "2025-06-21T14:51:35.716798Z",
     "shell.execute_reply": "2025-06-21T14:51:35.716480Z",
     "shell.execute_reply.started": "2025-06-21T14:51:35.716465Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- This section re-uses the feature engineering from Option 1 to get the polarity score ---\n",
    "\n",
    "# Define Lexicons and Helper Functions (same as before)\n",
    "singlish_happy = {'patta': 2, 'maru': 2, 'niyamai': 2, 'ela': 2, 'supiri': 2, 'gathi': 2, 'hodai': 1, 'lassanai': 1, 'shok': 1, 'jayawewa': 2, 'good':1, 'love':2}\n",
    "singlish_sad = {'apalai': -2, 'chaa': -2, 'anthimai': -2, 'weda na': -2, 'boring': -1, 'boru': -1, 'harak': -2, 'gon': -2, 'pissu': -2, 'sad':-1,'bad':-1}\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None: return \"\"\n",
    "    text = re.sub(r'http\\S+', '', text); text = re.sub(r'@\\w+', '', text); text = re.sub(r'#\\w+', '', text)\n",
    "    text = emoji.demojize(text); text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def calculate_custom_polarity(text):\n",
    "    score = 0; words = text.split()\n",
    "    for word in words: score += singlish_happy.get(word, 0); score += singlish_sad.get(word, 0)\n",
    "    return score\n",
    "\n",
    "# Load data\n",
    "df = pl.read_csv('/kaggle/input/tweet-politicians/sri_lanka_election_tweets_analysis_ready.csv')\n",
    "\n",
    "# Engineer the hybrid_polarity score\n",
    "df_featured = df.with_columns(\n",
    "    pl.col(\"original_text\").map_elements(clean_text, return_dtype=pl.String).alias(\"clean_text\")\n",
    ").with_columns(\n",
    "    pl.col(\"clean_text\").map_elements(calculate_custom_polarity, return_dtype=pl.Float64).alias(\"custom_polarity\"),\n",
    "    pl.col(\"clean_text\").map_elements(lambda x: TextBlob(x).sentiment.polarity, return_dtype=pl.Float64).alias(\"textblob_polarity\")\n",
    ").with_columns(\n",
    "    ((pl.col(\"textblob_polarity\") * 0.4) + (pl.col(\"custom_polarity\") * 0.6)).alias(\"hybrid_polarity\")\n",
    ")\n",
    "\n",
    "# --- NEW PART: Create categorical labels ---\n",
    "# We define thresholds to convert the continuous polarity score into discrete labels.\n",
    "def label_sentiment(polarity):\n",
    "    if polarity > 0.1:  # Threshold for positive\n",
    "        return 2  # Corresponds to 'positive'\n",
    "    elif polarity < -0.1:  # Threshold for negative\n",
    "        return 0  # Corresponds to 'negative'\n",
    "    else:\n",
    "        return 1  # Corresponds to 'neutral'\n",
    "\n",
    "# Map polarity to integer labels\n",
    "df_labeled = df_featured.with_columns(\n",
    "    pl.col(\"hybrid_polarity\").map_elements(label_sentiment, return_dtype=pl.Int64).alias(\"label\")\n",
    ").select([\"clean_text\", \"label\"]).drop_nulls()\n",
    "\n",
    "\n",
    "# Check the distribution of our new labels\n",
    "print(\"Label Distribution:\")\n",
    "print(df_labeled['label'].value_counts())\n",
    "\n",
    "# Convert to pandas for scikit-learn and TensorFlow compatibility\n",
    "pandas_df = df_labeled.to_pandas()\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = pandas_df['clean_text']\n",
    "y = pandas_df['label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "44c48231-2846-42fb-96a4-e6c7a6d161ac",
    "_uuid": "69ecda72-5554-4445-a0df-0b68e5d45742",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-06-21T14:51:35.718954Z",
     "iopub.status.idle": "2025-06-21T14:51:35.719537Z",
     "shell.execute_reply": "2025-06-21T14:51:35.719124Z",
     "shell.execute_reply.started": "2025-06-21T14:51:35.719110Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The model we are going to fine-tune\n",
    "MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "\n",
    "# Load the tokenizer associated with this model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load the model itself.\n",
    "# We specify num_labels=3 so the model creates a classification head\n",
    "# with 3 outputs (Negative, Neutral, Positive).\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "\n",
    "# The pre-trained model has a label mapping we can use for clarity\n",
    "LABEL_MAP = model.config.id2label\n",
    "print(\"Model's label mapping:\", LABEL_MAP)\n",
    "# Our labeling function was designed to match this: 0 -> negative, 1 -> neutral, 2 -> positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "978efe6a-ad19-44bd-aff1-ce1ab83209d4",
    "_uuid": "bb31bcca-90d2-41c7-8e8e-b42518958a36",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-06-21T14:51:35.720235Z",
     "iopub.status.idle": "2025-06-21T14:51:35.720963Z",
     "shell.execute_reply": "2025-06-21T14:51:35.720385Z",
     "shell.execute_reply.started": "2025-06-21T14:51:35.720372Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the text data for both training and testing sets\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create TensorFlow datasets, which are highly efficient for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train.tolist()\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test.tolist()\n",
    "))\n",
    "\n",
    "# Batch and prefetch the datasets for performance\n",
    "BATCH_SIZE = 16 # Use 8 or 16 for fine-tuning, depending on GPU memory\n",
    "train_dataset = train_dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\n✅ Datasets are ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1c28682d-4580-4e97-ac13-878ccbf3c167",
    "_uuid": "b11bb147-fe71-416c-b35a-69ab6374d5ac",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-06-21T14:51:35.721966Z",
     "iopub.status.idle": "2025-06-21T14:51:35.722674Z",
     "shell.execute_reply": "2025-06-21T14:51:35.722126Z",
     "shell.execute_reply.started": "2025-06-21T14:51:35.722112Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use the AdamW optimizer, recommended for Transformers\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=2e-5)\n",
    "\n",
    "# We must compile the model with from_logits=True because the model outputs raw scores (logits),\n",
    "# not probabilities (which a softmax function would produce).\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define a callback for early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5, # Fine-tuning usually requires fewer epochs\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Model fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "07665e36-2420-4fd0-ae38-f7a35a88b8e7",
    "_uuid": "8168abb4-aa5e-4db5-8eef-e6344491e1ea",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-06-21T14:51:35.723559Z",
     "iopub.status.idle": "2025-06-21T14:51:35.724127Z",
     "shell.execute_reply": "2025-06-21T14:51:35.723736Z",
     "shell.execute_reply.started": "2025-06-21T14:51:35.723721Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# First, get the model's raw predictions (logits) on the test set\n",
    "test_logits = model.predict(test_dataset).logits\n",
    "\n",
    "# Convert the logits to class predictions by taking the argmax\n",
    "y_pred = np.argmax(test_logits, axis=1)\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=LABEL_MAP.values()))\n",
    "\n",
    "# Generate and plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=LABEL_MAP.values(),\n",
    "            yticklabels=LABEL_MAP.values())\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c71a590d-bea6-441f-9c5f-d762f872fd16",
    "_uuid": "fc479285-1533-4d3f-bd46-533fefae532c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-21T15:39:27.421734Z",
     "iopub.status.busy": "2025-06-21T15:39:27.421253Z",
     "iopub.status.idle": "2025-06-21T15:39:27.682229Z",
     "shell.execute_reply": "2025-06-21T15:39:27.676980Z",
     "shell.execute_reply.started": "2025-06-21T15:39:27.421679Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Add this as a new cell at the end of your notebook ---\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Goal: Aggregate the predicted scores for each politician in the test set.\n",
    "\n",
    "# Step 1: Create a consolidated DataFrame with all necessary information.\n",
    "# The 'X_test' DataFrame has the 'Politician' column. 'y_test' has the true score.\n",
    "# 'y_pred_xgb' (a numpy array) has our predicted scores.\n",
    "\n",
    "# It's crucial to align the indices to ensure data integrity.\n",
    "results_df = X_test.copy()\n",
    "results_df['true_impact_score'] = y_test\n",
    "results_df['predicted_impact_score'] = y_pred_xgb\n",
    "\n",
    "print(\"--- Sample of the Results DataFrame ---\")\n",
    "display(results_df.head())\n",
    "\n",
    "\n",
    "# Step 2: Use groupby() to aggregate the scores for each politician.\n",
    "# We will calculate the total impact, the average impact per tweet, and the number of tweets.\n",
    "politician_summary = results_df.groupby('target_politicians').agg(\n",
    "    total_predicted_impact=('predicted_impact_score', 'sum'),\n",
    "    average_predicted_impact=('predicted_impact_score', 'mean'),\n",
    "    tweet_count=('target_politicians', 'count')\n",
    ").sort_values('total_predicted_impact', ascending=False) # Sort by who has the most impact\n",
    "\n",
    "print(\"\\n--- Aggregated Impact Summary per Politician ---\")\n",
    "display(politician_summary)\n",
    "\n",
    "\n",
    "# Step 3: Visualize the results for a clear comparison.\n",
    "# A bar chart is perfect for showing the total predicted impact.\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(\n",
    "    x=politician_summary.index,\n",
    "    y=politician_summary['total_predicted_impact'],\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Total Predicted Social Media Impact per Politician (on Test Set)', fontsize=16)\n",
    "plt.xlabel('Politician', fontsize=12)\n",
    "plt.ylabel('Total Predicted Impact Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMr4tlqZ9Z2EM3TfIba6bJD",
   "gpuType": "V28",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 7684496,
     "sourceId": 12199218,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
